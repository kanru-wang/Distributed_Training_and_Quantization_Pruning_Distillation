{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanru-wang/coursera_quantization_pruning_distillation/blob/main/Quantization_and_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3-cg2_rYfe6"
      },
      "source": [
        "# Quantization and Pruning\n",
        "\n",
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sL5kmRZbZxX"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "import zipfile"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS5gXwABm7XP"
      },
      "source": [
        "<a name='utilities'>\n",
        "\n",
        "## Utilities and constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEuiXyPZMKQm"
      },
      "source": [
        "# GLOBAL VARIABLES\n",
        "\n",
        "# String constants for model filenames\n",
        "FILE_WEIGHTS = 'baseline_weights.h5'\n",
        "FILE_NON_QUANTIZED_H5 = 'non_quantized.h5'\n",
        "FILE_NON_QUANTIZED_TFLITE = 'non_quantized.tflite'\n",
        "FILE_PT_QUANTIZED = 'post_training_quantized.tflite'\n",
        "FILE_QAT_QUANTIZED = 'quant_aware_quantized.tflite'\n",
        "FILE_PRUNED_MODEL_H5 = 'pruned_model.h5'\n",
        "FILE_PRUNED_QUANTIZED_TFLITE = 'pruned_quantized.tflite'\n",
        "FILE_PRUNED_NON_QUANTIZED_TFLITE = 'pruned_non_quantized.tflite'\n",
        "\n",
        "# Dictionaries to hold measurements\n",
        "MODEL_SIZE = {}\n",
        "ACCURACY = {}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqdSGWccdk8G"
      },
      "source": [
        "# UTILITY FUNCTIONS\n",
        "\n",
        "def print_metric(metric_dict, metric_name):\n",
        "    '''Prints key and values stored in a dictionary'''\n",
        "    for metric, value in metric_dict.items():\n",
        "        print(f'{metric_name} for {metric}: {value}')\n",
        "\n",
        "\n",
        "def model_builder():\n",
        "    '''Returns a shallow CNN for training on the MNIST dataset'''\n",
        "    keras = tf.keras\n",
        "    # Define the model architecture.\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "        keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "        keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Flatten(),\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_tflite_model(filename, x_test, y_test):\n",
        "    '''\n",
        "    Measures the accuracy of a given TF Lite model and test set\n",
        "  \n",
        "    Args:\n",
        "        filename (string) - filename of the model to load\n",
        "        x_test (numpy array) - test images\n",
        "        y_test (numpy array) - test labels\n",
        "\n",
        "    Returns\n",
        "        float showing the accuracy against the test set\n",
        "    '''\n",
        "\n",
        "    # Initialize the TF Lite Interpreter and allocate tensors\n",
        "    interpreter = tf.lite.Interpreter(model_path=filename)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input and output index\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "    # Initialize empty predictions list\n",
        "    prediction_digits = []\n",
        "  \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    for i, test_image in enumerate(x_test):\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "\n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = interpreter.tensor(output_index)\n",
        "        digit = np.argmax(output()[0])\n",
        "        prediction_digits.append(digit)\n",
        "\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == y_test).mean()\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def get_gzipped_model_size(file):\n",
        "    '''Returns size of gzipped model, in bytes.'''\n",
        "    _, zipped_file = tempfile.mkstemp('.zip')\n",
        "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "        f.write(file)\n",
        "\n",
        "    return os.path.getsize(zipped_file)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxnjOqLpYawi"
      },
      "source": [
        "## Download and Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5f5Y08r0sob",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bcdc2129-30cb-417e-b60e-3830d7485d32"
      },
      "source": [
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czvt9P1EYnQT"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ild5juYXu4j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c95c78d0-c05f-44fc-d662-e711da5a3824"
      },
      "source": [
        "# Create the baseline model\n",
        "baseline_model = model_builder()\n",
        "\n",
        "# Save the initial weights for use later\n",
        "baseline_model.save_weights(FILE_WEIGHTS)\n",
        "\n",
        "# Print the model summary\n",
        "baseline_model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 12)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2028)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,410\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xViB61FuY0Pf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8394bae3-5222-43ec-b77e-3bf8a956cb05"
      },
      "source": [
        "# Setup the model for training\n",
        "baseline_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "baseline_model.fit(train_images, train_labels, epochs=1, shuffle=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 38s 19ms/step - loss: 0.2656 - accuracy: 0.9275\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdd89657af0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQSVh1_t4Z2h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5cf453e3-5c47-4fe7-8a48-e30e37194d70"
      },
      "source": [
        "# Get the baseline accuracy\n",
        "_, ACCURACY['baseline Keras model'] = baseline_model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 0.1288 - accuracy: 0.9620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A8WPjzqLbH3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2a364f2c-8aac-4ada-b104-01b5d095180b"
      },
      "source": [
        "# Save the Keras model\n",
        "baseline_model.save(FILE_NON_QUANTIZED_H5, include_optimizer=False)\n",
        "\n",
        "# Save and get the model size\n",
        "MODEL_SIZE['baseline h5'] = os.path.getsize(FILE_NON_QUANTIZED_H5)\n",
        "\n",
        "# Print records so far\n",
        "print_metric(ACCURACY, \"test accuracy\")\n",
        "print_metric(MODEL_SIZE, \"model size in bytes\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy for baseline Keras model: 0.9620000123977661\n",
            "model size in bytes for baseline h5: 98968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak8rBX-qX_KM"
      },
      "source": [
        "### Convert the model to TF Lite format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQYM0A0SgCNS"
      },
      "source": [
        "def convert_tflite(model, filename, quantize=False):\n",
        "    '''\n",
        "    Converts the model to TF Lite format and writes to a file\n",
        "\n",
        "    Args:\n",
        "        model (Keras model) - model to convert to TF Lite\n",
        "        filename (string) - string to use when saving the file\n",
        "        quantize (bool) - flag to indicate quantization\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    '''\n",
        "    \n",
        "    # Initialize the converter\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "    # Set for quantization if flag is set to True\n",
        "    if quantize:\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "    # Convert the model\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    # Save the model.\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(tflite_model)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQkC9plnP2pU"
      },
      "source": [
        "This is not yet quantized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H61feiOZkcI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7174488c-ecdd-43fd-a9e0-d89d2296978a"
      },
      "source": [
        "# Convert baseline model\n",
        "convert_tflite(baseline_model, FILE_NON_QUANTIZED_TFLITE)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REf-EaQlQoYZ"
      },
      "source": [
        "Slight decrease in model size when converting to `.tflite` format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmlNGwbCBo8v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "93c6195b-9aa0-48ea-8c21-a90bf5858144"
      },
      "source": [
        "MODEL_SIZE['non quantized tflite'] = os.path.getsize(FILE_NON_QUANTIZED_TFLITE)\n",
        "\n",
        "print_metric(MODEL_SIZE, 'model size in bytes')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size in bytes for baseline h5: 98968\n",
            "model size in bytes for non quantized tflite: 85012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp-ndoNSRnvX"
      },
      "source": [
        "*If there is a `Runtime Error: There is at least 1 reference to internal data in the interpreter in the form of a numpy array or slice.` , try re-running the cell.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQFkh5ukiiZE"
      },
      "source": [
        "ACCURACY['non quantized tflite'] = evaluate_tflite_model(FILE_NON_QUANTIZED_TFLITE, test_images, test_labels)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CplCOws3jaB0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "67d7fe2f-4e35-45dd-e9ef-38a76589a570"
      },
      "source": [
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy for baseline Keras model: 0.9620000123977661\n",
            "test accuracy for non quantized tflite: 0.962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6ilHiSGYCFL"
      },
      "source": [
        "### Post-Training Quantization\n",
        "\n",
        "Convert 32 bit representations (float) into 8 bits (integer) to reduce model size and achieve faster computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdWNTJ2J1OpL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2b88dc3e-b926-4386-cdfe-06dbba56be45"
      },
      "source": [
        "# Convert and quantize the baseline model\n",
        "convert_tflite(baseline_model, FILE_PT_QUANTIZED, quantize=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTFHf4Rw1bCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6380d3d5-2071-4a4a-a268-063dac588fca"
      },
      "source": [
        "# Get the model size\n",
        "MODEL_SIZE['post training quantized tflite'] = os.path.getsize(FILE_PT_QUANTIZED)\n",
        "\n",
        "print_metric(MODEL_SIZE, 'model size')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size for baseline h5: 98968\n",
            "model size for non quantized tflite: 85012\n",
            "model size for post training quantized tflite: 24256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYcBZduWVqOH"
      },
      "source": [
        "About 4X reduction in model size in the quantized version\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhEYoQ83-pT_"
      },
      "source": [
        "ACCURACY['post training quantized tflite'] = evaluate_tflite_model(FILE_PT_QUANTIZED, test_images, test_labels)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D0Srsjb_inn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a7e71e20-1c49-423d-8fea-ee0206d53498"
      },
      "source": [
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy for baseline Keras model: 0.9620000123977661\n",
            "test accuracy for non quantized tflite: 0.962\n",
            "test accuracy for post training quantized tflite: 0.9616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFf1DDVnYIes"
      },
      "source": [
        "## Quantization Aware Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37oAb7PuXK36"
      },
      "source": [
        "Doing quantization aware training before quantizing the model in order to preserve more accuracy. It simulates the loss of precision by inserting fake quant nodes in the model during training, so the model will learn to adapt with the loss of precision to get more accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WSt6OQGoNAt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e313caa2-e628-4d71-9a91-676a74dfd2b9"
      },
      "source": [
        "# Install the toolkit\n",
        "!pip install tensorflow_model_optimization"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/238.9 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow_model_optimization) (0.1.8)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.9/dist-packages (from tensorflow_model_optimization) (1.16.0)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.9/dist-packages (from tensorflow_model_optimization) (1.22.4)\n",
            "Installing collected packages: tensorflow_model_optimization\n",
            "Successfully installed tensorflow_model_optimization-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYHmeMihYjnB"
      },
      "source": [
        "If we need to pass in a model that is already trained, need to recompile before continue training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dGSpz0on2C4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a541b180-0c48-42da-d849-a4b139b03351"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# method to quantize a Keras model\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# Define the model architecture. Still the baseline model\n",
        "model_to_quantize = model_builder()\n",
        "\n",
        "# Reinitialize weights with saved file\n",
        "model_to_quantize.load_weights(FILE_WEIGHTS)\n",
        "\n",
        "# Quantize the model\n",
        "q_aware_model = quantize_model(model_to_quantize)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (None, 28, 28)           3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_reshape_1 (QuantizeWr  (None, 28, 28, 1)        1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWra  (None, 26, 26, 12)       147       \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Quan  (None, 13, 13, 12)       1         \n",
            " tizeWrapperV2)                                                  \n",
            "                                                                 \n",
            " quant_flatten_1 (QuantizeWr  (None, 2028)             1         \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_dense_1 (QuantizeWrap  (None, 10)               20295     \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,448\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 38\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmcaLaotZ7G7"
      },
      "source": [
        "The number of model params increased because of the nodes added by the `quantize_model()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl4jbjllomDw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "dd2f358b-0e4b-4194-d5e8-282caa7d6697"
      },
      "source": [
        "# Train the model\n",
        "q_aware_model.fit(train_images, train_labels, epochs=1, shuffle=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 40s 21ms/step - loss: 0.2677 - accuracy: 0.9264\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdd895d89a0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7rOuwM_ozI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0c5eb9f8-2a34-456e-c20e-ea35a7505624"
      },
      "source": [
        "# Reinitialize the dictionary\n",
        "ACCURACY = {}\n",
        "\n",
        "# Get the accuracy of the quantization aware trained model (not yet quantized)\n",
        "_, ACCURACY['quantization aware non-quantized'] = q_aware_model.evaluate(test_images, test_labels, verbose=0)\n",
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy for quantization aware non-quantized: 0.9614999890327454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6liE_Cp3rzAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4bbb4835-efea-4f4e-ac4e-8f4fa5d8be10"
      },
      "source": [
        "# Convert and quantize the model.\n",
        "convert_tflite(q_aware_model, FILE_QAT_QUANTIZED, quantize=True)\n",
        "\n",
        "# Get the accuracy of the quantized model\n",
        "ACCURACY['quantization aware quantized'] = evaluate_tflite_model(FILE_QAT_QUANTIZED, test_images, test_labels)\n",
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, reshape_1_layer_call_fn, reshape_1_layer_call_and_return_conditional_losses, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "/usr/local/lib/python3.9/dist-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy for quantization aware non-quantized: 0.9614999890327454\n",
            "test accuracy for quantization aware quantized: 0.9617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwvaMflTYNgo"
      },
      "source": [
        "## Pruning\n",
        "\n",
        "Zero out insignificant low magnitude weights. Making the weights sparse helps in compressing the model.\n",
        "\n",
        "Pass in the baseline model trained earlier. The number of model params increased because of the wrapper layers added by the pruning method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpqizJsKYPBA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2569992a-df86-43c0-831e-fc9c4c98d8e8"
      },
      "source": [
        "# Get the pruning method\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define pruning schedule.\n",
        "pruning_params = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=0.50,\n",
        "        final_sparsity=0.80,\n",
        "        begin_step=0,\n",
        "        end_step=end_step\n",
        "    )\n",
        "}\n",
        "\n",
        "# Pass in the trained baseline model\n",
        "model_for_pruning = prune_low_magnitude(baseline_model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_reshape  (None, 28, 28, 1)        1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d   (None, 26, 26, 12)       230       \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_max_poo  (None, 13, 13, 12)       1         \n",
            " ling2d (PruneLowMagnitude)                                      \n",
            "                                                                 \n",
            " prune_low_magnitude_flatten  (None, 2028)             1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_dense (  (None, 10)               40572     \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,805\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 20,395\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgmHaZI6fip_"
      },
      "source": [
        "Peek at the weights of one layer before pruning. After pruning, many of these will be zeroed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5ekdEBigB5l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7093efcf-f9de-4675-8a2d-206321de54ca"
      },
      "source": [
        "# Preview model weights\n",
        "model_for_pruning.weights[1]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 12) dtype=float32, numpy=\n",
              "array([[[[ 0.21670069,  0.14448787, -0.3921924 , -0.5839371 ,\n",
              "           0.27797788,  0.11577412, -0.39421183, -0.5164737 ,\n",
              "           0.03561811,  0.23253371,  0.43321967,  0.17293948]],\n",
              "\n",
              "        [[ 0.35277528,  0.47311407, -0.05976715, -0.52713066,\n",
              "           0.07559928,  0.22698393, -0.06401199, -0.3694725 ,\n",
              "          -0.11109661,  0.27704132,  0.13577738,  0.29972312]],\n",
              "\n",
              "        [[-0.00256522,  0.7086048 ,  0.37773892, -0.3767449 ,\n",
              "           0.08378106,  0.05992218, -0.09731199,  0.32304454,\n",
              "           0.03348943, -0.07085457, -0.7070701 ,  0.1516765 ]]],\n",
              "\n",
              "\n",
              "       [[[ 0.15074071, -0.2181239 , -0.4557513 , -0.07979532,\n",
              "           0.27309716,  0.22540969, -0.18514894, -0.6064833 ,\n",
              "          -0.20642167,  0.15235803,  0.5228574 , -0.05335016]],\n",
              "\n",
              "        [[-0.10594414,  0.13728   ,  0.14186494,  0.23395713,\n",
              "          -0.0799045 ,  0.1545502 , -0.41689992,  0.03875222,\n",
              "           0.2576298 ,  0.17784038,  0.17341286,  0.1563548 ]],\n",
              "\n",
              "        [[ 0.04662576,  0.32866722,  0.30234453,  0.18747154,\n",
              "           0.03255994, -0.0080367 , -0.5505315 ,  0.48336294,\n",
              "           0.26164937,  0.31533328, -0.6162668 ,  0.32893378]]],\n",
              "\n",
              "\n",
              "       [[[ 0.01754893, -0.816298  , -0.2486164 ,  0.45346406,\n",
              "          -0.237634  , -0.14838156,  0.6031666 , -0.4262226 ,\n",
              "           0.20407943, -0.0718467 ,  0.2960732 , -0.32956994]],\n",
              "\n",
              "        [[-0.12628104, -0.82287264,  0.07329808,  0.3272577 ,\n",
              "          -0.05736372,  0.21582443,  0.31416008,  0.06292493,\n",
              "           0.30081314, -0.11668592,  0.10636348, -0.23664278]],\n",
              "\n",
              "        [[-0.08429123, -0.54284436,  0.2918199 ,  0.16646436,\n",
              "          -0.0588641 ,  0.08337758,  0.26641923,  0.38389373,\n",
              "           0.25774804,  0.04359985, -0.7338926 , -0.15107375]]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XFwMRqpgbr0"
      },
      "source": [
        "Start re-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUCz6PL371Bx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6282e321-85d2-484a-bf9b-0661aa08acd8"
      },
      "source": [
        "# Callback to update pruning wrappers at each step\n",
        "callbacks = [\n",
        "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "]\n",
        "\n",
        "# Train and prune the model\n",
        "model_for_pruning.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    epochs=epochs,\n",
        "    validation_split=validation_split,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1688/1688 [==============================] - 22s 12ms/step - loss: 0.1428 - accuracy: 0.9627 - val_loss: 0.0963 - val_accuracy: 0.9743\n",
            "Epoch 2/2\n",
            "1688/1688 [==============================] - 18s 11ms/step - loss: 0.1062 - accuracy: 0.9694 - val_loss: 0.0856 - val_accuracy: 0.9785\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdd8901da30>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEExgy4hhXP-"
      },
      "source": [
        "Weights in the same layer after pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOK4TidJhXpT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e0801e7e-2364-47ec-973a-733f755c9b5c"
      },
      "source": [
        "# Preview model weights\n",
        "model_for_pruning.weights[1]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 12) dtype=float32, numpy=\n",
              "array([[[[-0.        ,  0.        , -0.        , -1.0433143 ,\n",
              "           0.        , -0.        , -0.9726265 , -1.0005678 ,\n",
              "          -0.        ,  0.        ,  0.74551606,  0.        ]],\n",
              "\n",
              "        [[ 0.93137616,  0.82501125, -0.        , -0.31208834,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        ,  0.        ]],\n",
              "\n",
              "        [[-0.        ,  1.1437249 ,  0.6497642 , -0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "           0.        , -0.        , -1.0637197 ,  0.        ]]],\n",
              "\n",
              "\n",
              "       [[[-0.        ,  0.        ,  0.        , -0.        ,\n",
              "           0.        , -0.        , -0.        , -1.0609056 ,\n",
              "           0.        ,  0.        ,  0.9937551 ,  0.        ]],\n",
              "\n",
              "        [[-0.        ,  0.        , -0.        , -0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        ,  0.        ]],\n",
              "\n",
              "        [[-0.        ,  0.        , -0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.96841425,  1.0813432 ,\n",
              "           0.        ,  0.9514026 , -0.88002676,  0.        ]]],\n",
              "\n",
              "\n",
              "       [[[ 0.        , -1.2379985 ,  0.        ,  1.1212347 ,\n",
              "           0.        ,  0.        ,  0.8767976 , -0.        ,\n",
              "           0.        ,  0.        , -0.        ,  0.        ]],\n",
              "\n",
              "        [[ 0.        , -1.0962892 ,  0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        ,  0.        ]],\n",
              "\n",
              "        [[-0.        , -0.6971308 ,  0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        , -0.        ,\n",
              "           0.        , -0.        , -0.66768545,  0.        ]]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5ckfDHLhhub"
      },
      "source": [
        "After pruning, remove the wrapper layers to have the same layers and params as the baseline model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbfLhZv68vwc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "90ed068c-e870-46cc-82fa-0f750a6f9d96"
      },
      "source": [
        "# Remove pruning wrappers\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "model_for_export.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 12)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2028)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,410\n",
            "Trainable params: 20,410\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtbPlo-kj9Ku"
      },
      "source": [
        "For the same model weights, the index is now different, because the wrappers were removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG6-aF9yiraG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5fd9456f-2904-4505-e2dc-acd4f830eb21"
      },
      "source": [
        "# Preview model weights (index 1 earlier is now 0 because pruning wrappers were removed)\n",
        "model_for_export.weights[0]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 12) dtype=float32, numpy=\n",
              "array([[[[-0.        ,  0.        , -0.        , -1.0433143 ,\n",
              "           0.        , -0.        , -0.9726265 , -1.0005678 ,\n",
              "          -0.        ,  0.        ,  0.74551606,  0.        ]],\n",
              "\n",
              "        [[ 0.93137616,  0.82501125, -0.        , -0.31208834,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        ,  0.        ]],\n",
              "\n",
              "        [[-0.        ,  1.1437249 ,  0.6497642 , -0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "           0.        , -0.        , -1.0637197 ,  0.        ]]],\n",
              "\n",
              "\n",
              "       [[[-0.        ,  0.        ,  0.        , -0.        ,\n",
              "           0.        , -0.        , -0.        , -1.0609056 ,\n",
              "           0.        ,  0.        ,  0.9937551 ,  0.        ]],\n",
              "\n",
              "        [[-0.        ,  0.        , -0.        , -0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        ,  0.        ]],\n",
              "\n",
              "        [[-0.        ,  0.        , -0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.96841425,  1.0813432 ,\n",
              "           0.        ,  0.9514026 , -0.88002676,  0.        ]]],\n",
              "\n",
              "\n",
              "       [[[ 0.        , -1.2379985 ,  0.        ,  1.1212347 ,\n",
              "           0.        ,  0.        ,  0.8767976 , -0.        ,\n",
              "           0.        ,  0.        , -0.        ,  0.        ]],\n",
              "\n",
              "        [[ 0.        , -1.0962892 ,  0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        ,  0.        ]],\n",
              "\n",
              "        [[-0.        , -0.6971308 ,  0.        , -0.        ,\n",
              "           0.        ,  0.        , -0.        , -0.        ,\n",
              "           0.        , -0.        , -0.66768545,  0.        ]]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR94MYxLkHfn"
      },
      "source": [
        "The pruned model has the same file size as the baseline_model when saved as H5, which is to be expected. The improvement will be noticeable after compressing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjjDMqJCTjqz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3aab1c36-89e1-4634-8271-c4edda57a7e8"
      },
      "source": [
        "# Save Keras model\n",
        "model_for_export.save(FILE_PRUNED_MODEL_H5, include_optimizer=False)\n",
        "\n",
        "# Get uncompressed model size of baseline and pruned models\n",
        "MODEL_SIZE = {}\n",
        "MODEL_SIZE['baseline h5'] = os.path.getsize(FILE_NON_QUANTIZED_H5)\n",
        "MODEL_SIZE['pruned non quantized h5'] = os.path.getsize(FILE_PRUNED_MODEL_H5)\n",
        "\n",
        "print_metric(MODEL_SIZE, 'model_size in bytes')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_size in bytes for baseline h5: 98968\n",
            "model_size in bytes for pruned non quantized h5: 98968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCEfa-LRleT_"
      },
      "source": [
        "The pruned model is about 3 times smaller, because the zeros can be compressed much more efficiently than the low magnitude weights before pruning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWQ_AgiX_yiP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5c310df0-f18b-4b41-9df0-d6f79c495183"
      },
      "source": [
        "# Get compressed size of baseline and pruned models\n",
        "MODEL_SIZE = {}\n",
        "MODEL_SIZE['baseline h5'] = get_gzipped_model_size(FILE_NON_QUANTIZED_H5)\n",
        "MODEL_SIZE['pruned non quantized h5'] = get_gzipped_model_size(FILE_PRUNED_MODEL_H5)\n",
        "\n",
        "print_metric(MODEL_SIZE, \"gzipped model size in bytes\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzipped model size in bytes for baseline h5: 78044\n",
            "gzipped model size in bytes for pruned non quantized h5: 25768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uByyx0L3mlYc"
      },
      "source": [
        "Can make the model even more lightweight by quantizing the pruned model. About 10X reduction in compressed model size as compared to the baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIY6n9XWCvt5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "57cbbb17-624a-41b9-eeac-30d20a80feea"
      },
      "source": [
        "# Convert and quantize the pruned model.\n",
        "pruned_quantized_tflite = convert_tflite(model_for_export, FILE_PRUNED_QUANTIZED_TFLITE, quantize=True)\n",
        "\n",
        "# Compress and get the model size\n",
        "MODEL_SIZE['pruned quantized tflite'] = get_gzipped_model_size(FILE_PRUNED_QUANTIZED_TFLITE)\n",
        "print_metric(MODEL_SIZE, \"gzipped model size in bytes\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzipped model size in bytes for baseline h5: 78044\n",
            "gzipped model size in bytes for pruned non quantized h5: 25768\n",
            "gzipped model size in bytes for pruned quantized tflite: 8401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4ytiH3ynIid"
      },
      "source": [
        "Accuracy remains good"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZBAdJmuWN0A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "057d5194-28f2-4d24-fbe2-b9b39590c580"
      },
      "source": [
        "# Get accuracy of pruned Keras and TF Lite models\n",
        "ACCURACY = {}\n",
        "\n",
        "_, ACCURACY['pruned model h5'] = model_for_pruning.evaluate(test_images, test_labels)\n",
        "ACCURACY['pruned and quantized tflite'] = evaluate_tflite_model(FILE_PRUNED_QUANTIZED_TFLITE, test_images, test_labels)\n",
        "\n",
        "print_metric(ACCURACY, 'accuracy')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0931 - accuracy: 0.9716\n",
            "accuracy for pruned model h5: 0.9715999960899353\n",
            "accuracy for pruned and quantized tflite: 0.9717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VpeJ6g4GWA1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}