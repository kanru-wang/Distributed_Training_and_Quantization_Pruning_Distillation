{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanru-wang/Distributed_Training_and_Quantization_Pruning_Distillation/blob/main/multi_gpu_mirrored_strategy_distributed_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9r1zkT6Todm"
      },
      "source": [
        "# Multi-GPU Mirrored Strategy\n",
        "\n",
        "Multi-GPU mirrored strategy distributed training for Fashion MNIST.\n",
        "\n",
        "There's only 1 device that is available, which is enough for demonstrating the distribution strategies.\n",
        "\n",
        "See: https://www.coursera.org/learn/custom-distributed-training-with-tensorflow/lecture/21zgD/multi-gpu-mirrored-strategy-code-walkthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fmYkSujnTodo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuvdJbMhTodp"
      },
      "source": [
        "## Setup Distribution Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnM-hraATodq",
        "outputId": "e0059716-f254-44ad-e4cb-146eac61ec1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        }
      ],
      "source": [
        "# Note that it generally has a minimum of 8 cores, but if the GPU has\n",
        "# less, will need to set this. E.g., if one of the GPUs only has 4 cores\n",
        "# os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\n",
        "\n",
        "# If the list of devices is not specified in the\n",
        "# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n",
        "# If you have *different* GPUs, will probably have to set up cross_device_ops like this\n",
        "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Xre1VQTodq"
      },
      "source": [
        "## Prepare the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1sYknVQETodq"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Adding a dimension to the array\n",
        "# Old train_images.shape == (60000, 28, 28)\n",
        "# New train_images.shape == (60000, 28, 28, 1)\n",
        "# Do so because the first layer in the model is a convolutional layer\n",
        "# and it requires a 4D input (batch_size, height, width, channels).\n",
        "# batch_size dimension will be added later on.\n",
        "train_images = train_images[..., None]\n",
        "test_images = test_images[..., None]\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "train_images = train_images / np.float32(255)\n",
        "test_images = test_images / np.float32(255)\n",
        "\n",
        "BUFFER_SIZE = len(train_images)\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n",
        "\n",
        "# Create Distributed Datasets from the datasets\n",
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWF8hGh7Todr"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bze5ZhrtTods"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD6q4wUaTods"
      },
      "source": [
        "## Configure custom training\n",
        "\n",
        "Instead of `model.compile()`, will do custom training within a strategy scope.\n",
        "\n",
        "If there are two GPUs, the printed output of `per_example_loss` should look like this:\n",
        "\n",
        "    Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(48,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
        "    Tensor(\"replica_1/sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(48,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n",
        "    \n",
        "Note in particular that `replica_0` isn't named in the `weighted_loss` -- the first is unnamed, the second is `replica_1` etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Km33o8dQTods"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    # Use sparse categorical crossentropy, but instead of having the loss function\n",
        "    # manage the map reduce (which is how the losses get aggregated) across GPUs for us,\n",
        "    # will do it ourselves with a simple algorithm.\n",
        "    # Set reduction to `none` so we can do the reduction afterwards and divide by global batch size.\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True,\n",
        "        reduction=tf.keras.losses.Reduction.NONE\n",
        "    )\n",
        "\n",
        "    def compute_loss(labels, predictions):\n",
        "        # Notice that per_example_loss will have an entry per GPU,\n",
        "        # i.e. the loss for each replica\n",
        "        per_example_loss = loss_object(labels, predictions)\n",
        "        print(per_example_loss)\n",
        "        # tf.print(per_example_loss)\n",
        "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n",
        "\n",
        "    # Reduce by averaging the losses\n",
        "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # Create the model within the scope\n",
        "    model = create_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBf750kATodt"
      },
      "source": [
        "## Train and Test Steps Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k0VOdqKP2NEz"
      },
      "outputs": [],
      "source": [
        "# `run` replicates the provided computation and runs it with the distributed input.\n",
        "@tf.function\n",
        "def distributed_train_step(dataset_inputs):\n",
        "    per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
        "    # tf.print(per_replica_losses.values)\n",
        "    # print(per_replica_losses)\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
        "\n",
        "def train_step(inputs):\n",
        "    images, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training=True)\n",
        "        loss = compute_loss(labels, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_accuracy.update_state(labels, predictions)\n",
        "    return loss\n",
        "\n",
        "#######################\n",
        "# Test Steps Functions\n",
        "#######################\n",
        "@tf.function\n",
        "def distributed_test_step(dataset_inputs):\n",
        "    return strategy.run(test_step, args=(dataset_inputs,))\n",
        "\n",
        "def test_step(inputs):\n",
        "    images, labels = inputs\n",
        "\n",
        "    predictions = model(images, training=False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "\n",
        "    test_loss.update_state(t_loss)\n",
        "    test_accuracy.update_state(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACqrK2XYTodt"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "We can now start training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAZwt7NNTodt",
        "outputId": "a5e3f011-8cb0-4ede-9dac-cc720c88e2b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(64,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(32,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n",
            "Epoch 1, Loss: 0.5183785557746887, Accuracy: 81.40666198730469, Test Loss: 0.3894541561603546, Test Accuracy: 86.29000091552734\n",
            "Epoch 2, Loss: 0.33887457847595215, Accuracy: 87.81832885742188, Test Loss: 0.3472968637943268, Test Accuracy: 87.95999908447266\n",
            "Epoch 3, Loss: 0.2911539673805237, Accuracy: 89.37332916259766, Test Loss: 0.31044477224349976, Test Accuracy: 88.73999786376953\n",
            "Epoch 4, Loss: 0.2592061758041382, Accuracy: 90.58000183105469, Test Loss: 0.29861727356910706, Test Accuracy: 89.18000030517578\n",
            "Epoch 5, Loss: 0.2356150597333908, Accuracy: 91.37667083740234, Test Loss: 0.27762946486473083, Test Accuracy: 89.52999877929688\n",
            "Epoch 6, Loss: 0.21634185314178467, Accuracy: 92.14833068847656, Test Loss: 0.2657307982444763, Test Accuracy: 90.34000396728516\n",
            "Epoch 7, Loss: 0.1970299482345581, Accuracy: 92.8066635131836, Test Loss: 0.2659880220890045, Test Accuracy: 90.37000274658203\n",
            "Epoch 8, Loss: 0.18360184133052826, Accuracy: 93.2683334350586, Test Loss: 0.25277069211006165, Test Accuracy: 90.81999969482422\n",
            "Epoch 9, Loss: 0.1691894233226776, Accuracy: 93.67832946777344, Test Loss: 0.26327547430992126, Test Accuracy: 90.66000366210938\n",
            "Epoch 10, Loss: 0.15441164374351501, Accuracy: 94.2683334350586, Test Loss: 0.26260530948638916, Test Accuracy: 90.83999633789062\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    # Do Training\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for batch in train_dist_dataset:\n",
        "        total_loss += distributed_train_step(batch)\n",
        "        num_batches += 1\n",
        "    train_loss = total_loss / num_batches\n",
        "\n",
        "    # Do Testing\n",
        "    for batch in test_dist_dataset:\n",
        "        distributed_test_step(batch)\n",
        "\n",
        "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\n",
        "\n",
        "    print(template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n",
        "\n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}