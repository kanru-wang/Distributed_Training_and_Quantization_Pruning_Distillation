{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanru-wang/Distributed_Training_and_Quantization_Pruning_Distillation/blob/main/multi_gpu_mirrored_distributed_train_flower.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Multi-GPU Mirrored Strategy Flower Image Classification\n",
        "\n",
        "The code is useful for both single device and multi-device setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM6W__qraV55"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7MqDQO0KCaWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a12c958-6545-4a7a-d6c9-59682ac6802a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 328.90 MiB (download: 328.90 MiB, generated: 331.34 MiB, total: 660.25 MiB) to data/oxford_flowers102/2.1.1...\n",
            "Dataset oxford_flowers102 downloaded and prepared to data/oxford_flowers102/2.1.1. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "splits = ['train[:80%]', 'train[80%:90%]', 'train[90%:]']\n",
        "\n",
        "(train_examples, validation_examples, test_examples), info = tfds.load(\n",
        "    'oxford_flowers102',\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        "    split = splits,\n",
        "    data_dir='data/'\n",
        ")\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mVuLZhbem8d"
      },
      "source": [
        "How does `tf.distribute.MirroredStrategy` strategy work?\n",
        "\n",
        "*   All the variables and the model graph are replicated on the replicas.\n",
        "*   Input is evenly distributed across the replicas.\n",
        "*   Each replica calculates the loss and gradients for the input it received.\n",
        "*   The gradients are synced across all the replicas by summing them.\n",
        "*   After the sync, the same update is made to the copies of the variables on each replica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F2VeZUWUj5S4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2baf7082-1c05-4a7a-f4ec-5db9fa508f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n"
          ]
        }
      ],
      "source": [
        "# If the list of devices is not specified in the\n",
        "# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k53F5I_IiGyI"
      },
      "source": [
        "Setup input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jwJtsCQhHK-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23dd909b-79cd-453d-dbc1-9a5b8a0d5027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using https://tfhub.dev/tensorflow/resnet_50/feature_vector/1 with input size (224, 224)\n"
          ]
        }
      ],
      "source": [
        "BUFFER_SIZE = num_examples\n",
        "EPOCHS = 10\n",
        "pixels = 224\n",
        "\n",
        "MODULE_HANDLE='https://tfhub.dev/tensorflow/resnet_50/feature_vector/1'\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
        "\n",
        "# Resizes the image and scales the pixel values to range from [0,1]\n",
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
        "    return  image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKQ6WW8B-x3d"
      },
      "source": [
        "Set the global batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mKUXWQkg-x3d"
      },
      "outputs": [],
      "source": [
        "def set_global_batch_size(batch_size_per_replica, strategy):\n",
        "    '''\n",
        "    Args:\n",
        "        batch_size_per_replica (int) - batch size per replica\n",
        "        strategy (tf.distribute.Strategy) - distribution strategy\n",
        "    '''\n",
        "    global_batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n",
        "    return global_batch_size\n",
        "\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = set_global_batch_size(BATCH_SIZE_PER_REPLICA, strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7fj3GskHC8g"
      },
      "source": [
        "Create the datasets using the global batch size and distribute the batches for training, validation and test batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WYrMNNDhAvVl"
      },
      "outputs": [],
      "source": [
        "train_batches = train_examples.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n",
        "validation_batches = validation_examples.map(format_image).batch(BATCH_SIZE_PER_REPLICA).prefetch(1)\n",
        "test_batches = test_examples.map(format_image).batch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRDIRmIl-x3e"
      },
      "source": [
        "Define the distributed datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ygxvVntw-x3e"
      },
      "outputs": [],
      "source": [
        "def distribute_datasets(strategy, train_batches, validation_batches, test_batches):\n",
        "    train_dist_dataset = strategy.experimental_distribute_dataset(train_batches)\n",
        "    val_dist_dataset = strategy.experimental_distribute_dataset(validation_batches)\n",
        "    test_dist_dataset = strategy.experimental_distribute_dataset(test_batches)\n",
        "    return train_dist_dataset, val_dist_dataset, test_dist_dataset\n",
        "\n",
        "\n",
        "train_dist_dataset, val_dist_dataset, test_dist_dataset = distribute_datasets(\n",
        "    strategy,\n",
        "    train_batches,\n",
        "    validation_batches,\n",
        "    test_batches\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y40ZX70-x3f"
      },
      "source": [
        "Each batch has 64 features and labels.\n",
        "\n",
        "Take a look at a single batch from the train_dist_dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hEwQUdg-x3f",
        "outputId": "43744de5-2050-4ca2-e716-2f8b8b83339e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x is a tuple that contains 2 values \n",
            "x[0] contains the features, and has shape (64, 224, 224, 3)\n",
            "  so it has 64 examples in the batch, each is an image that is (224, 224, 3)\n",
            "x[1] contains the labels, and has shape (64,)\n"
          ]
        }
      ],
      "source": [
        "x = iter(train_dist_dataset).get_next()\n",
        "\n",
        "print(f\"x is a tuple that contains {len(x)} values \")\n",
        "print(f\"x[0] contains the features, and has shape {x[0].shape}\")\n",
        "print(f\"  so it has {x[0].shape[0]} examples in the batch, each is an image that is {x[0].shape[1:]}\")\n",
        "print(f\"x[1] contains the labels, and has shape {x[1].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAXAo_wWbWSb"
      },
      "source": [
        "## Create the model\n",
        "\n",
        "Use the Model Subclassing API to create model `ResNetModel` as a subclass of `tf.keras.Model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9ODch-OFCaW4"
      },
      "outputs": [],
      "source": [
        "class ResNetModel(tf.keras.Model):\n",
        "    def __init__(self, classes):\n",
        "        super(ResNetModel, self).__init__()\n",
        "        self._feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                                 trainable=False)\n",
        "        self._classifier = tf.keras.layers.Dense(classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self._feature_extractor(inputs)\n",
        "        x = self._classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ovc9A0j-x3f"
      },
      "source": [
        "Create a checkpoint directory to store the checkpoints (the model's weights during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9iagoTBfijUz"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-wlFFZbP33n"
      },
      "source": [
        "## Define the loss function\n",
        "\n",
        "- `loss_object` for calculating the loss on the test set\n",
        "- `compute_loss` for calculating the average loss on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R144Wci782ix"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    # Set reduction to `NONE` so we can do the reduction afterwards and divide by\n",
        "    # the global batch size.\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        reduction=tf.keras.losses.Reduction.NONE\n",
        "    )\n",
        "    # or loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
        "    def compute_loss(labels, predictions):\n",
        "        per_example_loss = loss_object(labels, predictions)\n",
        "        return tf.nn.compute_average_loss(\n",
        "            per_example_loss,\n",
        "            global_batch_size=GLOBAL_BATCH_SIZE\n",
        "        )\n",
        "\n",
        "    test_loss = tf.keras.metrics.Mean(name='test_loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8y54-o9T2Ni"
      },
      "source": [
        "## Define the metrics\n",
        "\n",
        "Use `.result()` to get the accumulated statistics, e.g. `train_accuracy.result()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zt3AHb46Tr3w"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        name='train_accuracy'\n",
        "    )\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "        name='test_accuracy'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuKuNXPORfqJ"
      },
      "source": [
        "## Instantiate the model, optimizer, and checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OrMmakq5EqeQ"
      },
      "outputs": [],
      "source": [
        "# Model and optimizer must be created under `strategy.scope`.\n",
        "with strategy.scope():\n",
        "    model = ResNetModel(classes=num_classes)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoySePyN-x3m"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Define a regular training step and test step, which could work without a distributed strategy. Then use `strategy.run` to apply these functions in a distributed manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zUQ_nAP1MtA9"
      },
      "outputs": [],
      "source": [
        "def train_test_step_fns(strategy, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy):\n",
        "    with strategy.scope():\n",
        "        def train_step(inputs):\n",
        "            images, labels = inputs\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = model(images, training=True)\n",
        "                loss = compute_loss(labels, predictions)\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            train_accuracy.update_state(labels, predictions)\n",
        "            return loss\n",
        "\n",
        "        def test_step(inputs):\n",
        "            images, labels = inputs\n",
        "            predictions = model(images, training=False)\n",
        "            t_loss = loss_object(labels, predictions)\n",
        "            test_loss.update_state(t_loss)\n",
        "            test_accuracy.update_state(labels, predictions)\n",
        "\n",
        "        return train_step, test_step\n",
        "\n",
        "\n",
        "train_step, test_step = train_test_step_fns(\n",
        "    strategy, model, compute_loss, optimizer, train_accuracy, loss_object,\n",
        "    test_loss, test_accuracy\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gizvZhjj-x3n"
      },
      "source": [
        "## Distributed training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G1fyFGv_-x3o"
      },
      "outputs": [],
      "source": [
        "def distributed_train_test_step_fns(strategy, train_step, test_step, model, compute_loss, optimizer, train_accuracy, loss_object, test_loss, test_accuracy):\n",
        "    with strategy.scope():\n",
        "        @tf.function\n",
        "        def distributed_train_step(dataset_inputs):\n",
        "            per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n",
        "            return strategy.reduce(\n",
        "                tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                axis=None\n",
        "            )\n",
        "\n",
        "        @tf.function\n",
        "        def distributed_test_step(dataset_inputs):\n",
        "            return strategy.run(test_step, args=(dataset_inputs,))\n",
        "\n",
        "        return distributed_train_step, distributed_test_step\n",
        "\n",
        "\n",
        "distributed_train_step, distributed_test_step = distributed_train_test_step_fns(\n",
        "    strategy, train_step, test_step, model, compute_loss, optimizer,\n",
        "    train_accuracy, loss_object, test_loss, test_accuracy\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "The scaled loss is the return value of the `distributed_train_step`. This value is aggregated across replicas using the `tf.distribute.Strategy.reduce` call and then across batches by summing the return value of the `tf.distribute.Strategy.reduce` calls."
      ],
      "metadata": {
        "id": "910H-dS8NPxo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gX975dMSNw0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b589fec-c9b0-4a56-fdb6-d77b00161885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:21,  1.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.6524457931518555, Accuracy: 4.779411792755127, Test Loss: 3.83022403717041, Test Accuracy: 9.803921699523926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:05,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 2.5824027061462402, Accuracy: 50.0, Test Loss: 2.7955212593078613, Test Accuracy: 40.19607925415039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:02,  4.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 1.4408522844314575, Accuracy: 80.88235473632812, Test Loss: 2.1607375144958496, Test Accuracy: 59.80392074584961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:02,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 0.8598877191543579, Accuracy: 94.48529815673828, Test Loss: 1.8130377531051636, Test Accuracy: 62.74510192871094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:02,  4.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 0.5642362833023071, Accuracy: 97.05882263183594, Test Loss: 1.6388604640960693, Test Accuracy: 64.70588684082031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:02,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Loss: 0.3913654088973999, Accuracy: 98.52941131591797, Test Loss: 1.4864661693572998, Test Accuracy: 66.66667175292969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:02,  4.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Loss: 0.2857825756072998, Accuracy: 99.26470184326172, Test Loss: 1.4010533094406128, Test Accuracy: 65.68627166748047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:02,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Loss: 0.2221342921257019, Accuracy: 99.63235473632812, Test Loss: 1.3403692245483398, Test Accuracy: 65.68627166748047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:02,  4.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Loss: 0.1780683994293213, Accuracy: 99.75489807128906, Test Loss: 1.307769775390625, Test Accuracy: 65.68627166748047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "13it [00:05,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 0.14703938364982605, Accuracy: 99.87745666503906, Test Loss: 1.2656513452529907, Test Accuracy: 66.66667175292969\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    for epoch in range(EPOCHS):\n",
        "        # TRAIN LOOP\n",
        "        total_loss = 0.0\n",
        "        num_batches = 0\n",
        "        for x in tqdm(train_dist_dataset):\n",
        "            total_loss += distributed_train_step(x)\n",
        "            num_batches += 1\n",
        "        train_loss = total_loss / num_batches\n",
        "\n",
        "        # TEST LOOP\n",
        "        for x in test_dist_dataset:\n",
        "            distributed_test_step(x)\n",
        "\n",
        "        template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}\")\n",
        "        print(\n",
        "            template.format(\n",
        "                epoch+1,\n",
        "                train_loss,\n",
        "                train_accuracy.result()*100,\n",
        "                test_loss.result(),\n",
        "                test_accuracy.result()*100\n",
        "            )\n",
        "        )\n",
        "\n",
        "        test_loss.reset_states()\n",
        "        train_accuracy.reset_states()\n",
        "        test_accuracy.reset_states()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "coursera": {
      "schema_names": [
        "TF3C2W4-1",
        "TF3C2W4-2",
        "TF3C2W4-3",
        "TF3C2W4-4"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}